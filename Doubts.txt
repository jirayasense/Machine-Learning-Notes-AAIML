Doubts

1) Why to find minima we do double derivative ?? (Live Session Programming 1.3)

2) Why CDF can't be used directly when Distribution is not Gaussian

3) Q Actual Diff between Kernal Density Plot vs Normal PDF or CDF 
   (let say for Uniform Distb)


4) If two random variables are related in a deterministic way, how are the PDFs related?  
   (from prob stats interview que)


5) In NB, Problem 2 of Outliers 
   (ie very few words in training dataset)

   So as a Soln do we need to keep thresold of word keeping in mind that word cnt per classes ?
      or on entire training set

      because if data is skewed then all words fall in same classes & we would not know if we 
      we compare word-freq before class groupings !
      

6) In Logistic Regression Why we assume that when sigmoid gives Probaility Yi=1 then its belonging to +1 & not -1
   Can't we take -1 class points in direction of Yi=1
     &           +1 class pts in direction of Yi=0


7) Does 
   Sigmoid gives probability for truly points in range 0.5 -> 1
   &
   For miss-classified points from 0 -> 0.5

8) So When sigmoid gives probability Yi = 1, here 1 is not class point right it can be for +1 or -1 both right ??

9) In logistic Regression We assume Normal W to be Unit Vector
   then why should we care abt Weight Vector W as W will be comprised of all basis vectors

   (Does it mean that we assume unit vector )

9.1) In Langrange Multiplier We constrained W^T*W = 1 i.e W to be unit vector

10) As Dimen is very high, How data is linearly seperable (probability incr) ??
   (from Log-Reg Real World Cases)


11) Does Gradient Descent is applied on entire Optimization Proble 
    (ie Loss Term  + Regularization Term)
    Or just apply first considering Loss term & then once optimized 
    accounted the Regularization term later

    (If does this way then do chances of Overfitting increases  ???)


---------------------

(Resource to ask for AAIML)

-> SQL commands.txt	
