algo ML-2 Imp Points

- Hyper-param & CV
 ------ 
Train - Test Split :
     \
      Grid-SearchCV for finding best split ratio or optimal hyper-parameter (via Cross Validation)

----
 KNN :- instance based method
 NB  :- Probabilistic method
 Log-Reg, Linear-Reg, SVM  :- Geometry Inspired

---

* Linear Model : Model which possess linear surface as seperating one are called as Linear Model
      Eg Linear Reg, Logistic Reg, Lr-SVM

* Logistic Reg :
  ----
   -> Is Gaussian naive bayes

* Convex Hull for SVM : 

* SVM :- 
    RBF kernel imp   // intution similar to KNN

    Algo to implement Dual Form of SVM :- SMO (Sequential Minimal Optimization)

* DT
  
  => Feature will only be imp or useful when it reduces the entropy going down level in DT
  or 
  => If IG due to that feature fi is maximum overall whilst constructing tree then that feature is 
     imp

* Enesemble Models :
  
  - Bagging (to work more for generalization part ie lowering variance)
  - Boosting (to work for Loss part i.e lowering bias ie training err)
  - Stacking Models
  - Cascade Models

* Gradient Descent :
  ------------
   -> To approximate the optimization of a variable via update steps
   - Eg Linear Regression

  Steepest Descent :
  ------
   -> Similar concept to GD, where functions are optimized instead of variable
   - Eg pseudo-residual

=> Linear Model have tendency towards High Bias Problem

=> Response Coding is 1 of technique available in case of Multi Class Classification

* Hyper Param Tuning :
  --------
   - is performed via CV dataset
   - Its often performed via GridsearchCV or RandomSearchCV
   - It involves trying out diff param value for Hyper-Parameters


* (REMEMBRANCE)----

 1) The vocabuylary should be built only with the words of Train Data

 2) Data Leakage Problem :
    ---------------
      - Dont do fit_transform() before train-test-split

      - at the time of training we are giving some hints abt some features which are new to train
        but not new to test

 3) You should pass the probability scores & not the predicted values for ROC-AUC calculations

* BOW vs One-Hot :
  -----------
   BOW :- considers the actual count
   One-Hot :- 1 or 0 ie present or not present (ie Binary BOW)


* Given Probabilities Score (prediction) & CM :
  --------------
   => Given Probabilities Score if you want to find the Confusion Matrix then 
      first you need to find the optimal thresold out of probability 
        \
         ie its not always necessary that 0.5 is optimal thresolds always

-> Sampling with Replacement & Without Replacement have some implications with Ennsemble methods
   ie Bagginng & BootStrapping
      
