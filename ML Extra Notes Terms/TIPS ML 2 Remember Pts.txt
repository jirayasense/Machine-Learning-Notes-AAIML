TIPS ML Remember Pts.txt

-------------------------

Imp Concepts :- Projection / Distance / Dot Product / Angular Similarity

				Differentiation / slope / tangent / Vector Calculus / Partial Derivative / Gradient Descent/ 
				Objective function

				
------------------------

-> Feature Collinearity Test :- Use Pertubation Technique

=> As Dimen is very high, data is linearly seperable (probability incr)

-> If you want to transform your features 
    then you cann use several mathematical intutions

       1) square   // to get positive
       2) Log 
       3) Exponent
       4) Sigmoid

=> there is no problem of imbalanced data in linear regression as there are no class labels.

=> One of the most imp algorithm for Optimization in ML is Stochastic Gradient Descent (SGD)

-> Regularization can be thought of as an imposing an equality constraint & solving that via 
   Langrangian Multipliers

=> The sparsity do not depends on the loss but on regularizer

=< For regression we use Mean Squared Error (MSE) or Median Absolute Deviation (MAD)

=> Bagging helps use to reduce the Variance of a Model

=> Thus approximating the residual with pseudo-residuals we can optimize our problem for any loss func

=> Hinge Loss has small problem i.e at Hinge its not differentiable

=> For any model, always use Cross Validation to get Hyper-param

=> Use GBDT impl of XGBoost instead of Scikit learn implementation

=> GBDT is applied more than AdABoost esp at internet companies

=> When the cost of making a mistake is very high :- Cascade models are used typically

-> Ensemble Model is used very much in kaggle Competition